{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:08.210372Z",
     "start_time": "2019-10-18T23:05:55.069377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:36.379053Z",
     "start_time": "2019-10-18T23:06:36.376481Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "data_dir = \"/Users/yangsong/Desktop/Projects/gitrepo_songyang0716/Topic_Modeling/reviews_small.txt\"\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:36.958783Z",
     "start_time": "2019-10-18T23:06:36.902827Z"
    }
   },
   "outputs": [],
   "source": [
    "# read review texts\n",
    "reviews = []\n",
    "f = open(data_dir, \"r\")\n",
    "for review in f:\n",
    "    reviews.append(review)\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.072839Z",
     "start_time": "2019-10-18T23:06:37.470766Z"
    }
   },
   "outputs": [],
   "source": [
    "# process text\n",
    "# tokenize, lower, remove stop words, stem, then only keep alphabets in the string\n",
    "clean_reviews = []\n",
    "for review in reviews:\n",
    "    s = nltk.word_tokenize(review)\n",
    "    s = [word.lower() for word in s]\n",
    "    s = [word for word in s if not word in set(\n",
    "        nltk.corpus.stopwords.words('english'))]\n",
    "    s = [stemmer.stem(word) for word in s if word.isalpha()]\n",
    "    clean_reviews.append(\" \".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.106261Z",
     "start_time": "2019-10-18T23:06:40.102231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.140277Z",
     "start_time": "2019-10-18T23:06:40.135118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is the second time I've used APlus for plumbing issues. Juan and Miguel once again resolved our problem with ease and efficiency. I was happy to see them waiting for me once I got home from work at 4:07p for my 4p-6 appointment window.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.179739Z",
     "start_time": "2019-10-18T23:06:40.175568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'second time use aplus plumb issu juan miguel resolv problem eas effici happi see wait got home work appoint window'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T01:59:44.628559Z",
     "start_time": "2019-10-21T01:59:44.552303Z"
    }
   },
   "outputs": [],
   "source": [
    "biterms = []\n",
    "unique_words = set()\n",
    "for clean_review in clean_reviews:\n",
    "    clean_review = clean_review.split()\n",
    "    review_length = len(clean_review)\n",
    "    cur_review_biterms = set()\n",
    "    for i in range(review_length):\n",
    "        unique_words.add(clean_review[i])\n",
    "        for j in range(i+1, review_length):\n",
    "            cur_review_biterms.add((clean_review[i], clean_review[j]))\n",
    "    biterms.extend(list(cur_review_biterms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T01:59:45.412604Z",
     "start_time": "2019-10-21T01:59:45.408702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104242"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:02:31.212318Z",
     "start_time": "2019-10-21T02:02:31.209614Z"
    }
   },
   "outputs": [],
   "source": [
    "n_wz = np.zeros((len(unique_words), num_of_topics), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:02:34.056133Z",
     "start_time": "2019-10-21T02:02:34.051466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_wz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(C) YANG SONG - 2019\n",
    "Implementation of the collapsed Gibbs sampler for\n",
    "Biterm Topic Models, as described in\n",
    "Biterm Topic Model for Short Texts (Yan,  Guo, Lan, Cheng)\n",
    "\"\"\"\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "data_dir = \"/Users/yangsong/Desktop/Projects/Topic_Modeling/reviews_small.txt\"\n",
    "np.random.seed(666)\n",
    "\n",
    "# read review texts\n",
    "reviews = []\n",
    "f = open(data_dir, \"r\")\n",
    "for review in f:\n",
    "    reviews.append(review)\n",
    "random.shuffle(reviews)\n",
    "\n",
    "# process text\n",
    "# tokenize, lower, remove stop words, stem, then only keep alphabets in the string\n",
    "clean_reviews = []\n",
    "for review in reviews:\n",
    "    s = nltk.word_tokenize(review)\n",
    "    s = [word.lower() for word in s]\n",
    "    s = [word for word in s if not word in set(\n",
    "        nltk.corpus.stopwords.words('english'))]\n",
    "    s = [stemmer.stem(word) for word in s if word.isalpha()]\n",
    "    clean_reviews.append(s)\n",
    "\n",
    "# extract all the unique biterms from the reviews\n",
    "# BTM directly models the word cooccurrence patterns based on biterms\n",
    "# A biterm denotes an unordered unique word-pair co-occurring in a short context, each context in our example is a review\n",
    "biterms = []\n",
    "unique_words = set()\n",
    "for clean_review in clean_reviews:\n",
    "    clean_review = clean_review.split()\n",
    "    review_length = len(clean_review)\n",
    "    cur_review_biterms = set()\n",
    "    for i in range(review_length):\n",
    "        unique_words.add(clean_review[i])\n",
    "        for j in range(i+1, review_length):\n",
    "            cur_review_biterms.add((clean_review[i], clean_review[j]))\n",
    "    biterms.extend(list(cur_review_biterms))\n",
    "\n",
    "# bigrams only\n",
    "# biterms = [biterm for review in clean_reviews for biterm in zip(review.split(\" \")[:-1], review.split(\"\")[1:])]\n",
    "# biterms = set(biterms)\n",
    "\n",
    "\n",
    "def BTM(reviews, biterms, unique_words, num_of_topics, num_of_iterations):\n",
    "    ####################################################################################\n",
    "    ### reviews: contains a list of reviews, and each review is a list of words      ###\n",
    "    ### num_of_topics: number of topics to generate                                  ###\n",
    "    ### number_of_iterations: collapsed gibbs sampling iterations                    ###\n",
    "    ####################################################################################\n",
    "\n",
    "    # constant we set for the LD prior (topic distributions in a document)\n",
    "    DL_ALPHA = 50 / num_of_topics\n",
    "    # constant we set for the LD prior (word distribution in a topic)\n",
    "    DL_BETA = 0.01\n",
    "    # Number of total biterms\n",
    "    N_BITERMS = len(biterms)\n",
    "\n",
    "    # Assign a random topic for each biterm\n",
    "    n_z = np.random.randint(0, num_of_topics, N_BITERMS)\n",
    "    # Words count over topics\n",
    "    # Key is word, value is an array of topic counts, use the index to indicate the topic 1 to k\n",
    "    n_wz = defaultdict(lambda: np.zeros(num_of_topics))\n",
    "    for index, (w1, w2) in enumerate(biterms):\n",
    "        n_wz[w1][n_z[index]] += 1\n",
    "        n_wz[w2][n_z[index]] += 1\n",
    "\n",
    "    # unlike to LDA model, in the biterm model, each bigram is coming from a specific topic\n",
    "    # biterm_topic = np.zeros((N_BITERMS, num_of_topics))\n",
    "    for iteration in range(num_of_iterations):\n",
    "        for index, (w1, w2) in enumerate(biterms):\n",
    "            # give a -1 class to the current biterm, means we ignore the current biterm\n",
    "            n_z[index] = -1\n",
    "            nz = np.unique(n_z, return_counts=True)[1][1:]\n",
    "            n_w1z = n_wz[w1]\n",
    "            n_w2z = n_wz[w2]\n",
    "\n",
    "            z_posterior = (nz + DL_ALPHA) * (n_wz[w1] + DL_BETA) * (n_wz[w2] + DL_BETA) / np.sum(\n",
    "                (2 * nz + len(unique_words) * DL_BETA) * (2 * nz + len(unique_words) * DL_BETA))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

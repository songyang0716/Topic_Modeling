{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T05:55:31.218431Z",
     "start_time": "2019-11-07T05:55:16.691350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T05:56:01.894924Z",
     "start_time": "2019-11-07T05:56:01.859018Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "data_dir = \"/Users/yangsong/Desktop/Projects/gitrepo_songyang0716/Topic_Modeling/reviews_small.txt\"\n",
    "np.random.seed(666)\n",
    "\n",
    "# read review texts\n",
    "reviews = []\n",
    "f = open(data_dir, \"r\")\n",
    "for review in f:\n",
    "    reviews.append(review)\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T05:56:05.900122Z",
     "start_time": "2019-11-07T05:56:02.965706Z"
    }
   },
   "outputs": [],
   "source": [
    "# process text\n",
    "# tokenize, lower, remove stop words, stem, then only keep alphabets in the string\n",
    "clean_reviews = []\n",
    "for review in reviews:\n",
    "    s = nltk.word_tokenize(review)\n",
    "    s = [word.lower() for word in s]\n",
    "    s = [word for word in s if not word in set(\n",
    "        nltk.corpus.stopwords.words('english'))]\n",
    "    s = [stemmer.stem(word) for word in s if word.isalpha()]\n",
    "    clean_reviews.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T01:03:30.077086Z",
     "start_time": "2019-11-09T01:03:30.072679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T01:03:35.000978Z",
     "start_time": "2019-11-09T01:03:34.997732Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set([word for review in clean_reviews for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T01:03:42.472013Z",
     "start_time": "2019-11-09T01:03:42.467198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T02:43:34.156627Z",
     "start_time": "2019-11-09T02:43:34.138580Z"
    }
   },
   "outputs": [],
   "source": [
    "def plsa(clean_reviews, num_of_topics, num_of_iterations, num_of_unique_words):\n",
    "\t####################################################################################\n",
    "\t### clean_reviews: clean reviews that has been tokenized, store in a list        ###\n",
    "\t### num_of_topics: number of topics to generate                                  ###\n",
    "\t### number_of_iterations: collapsed gibbs sampling iterations                    ###\n",
    "\t####################################################################################\n",
    "\t# words dictionary\n",
    "\tword_index = {}\n",
    "\tindex_word = {}\n",
    "\tindex = 0\n",
    "\tfor review in clean_reviews:\n",
    "\t\tfor word in review:\n",
    "\t\t\tif word in word_index:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_index[word] = index\n",
    "\t\t\t\tindex_word[index] = word\n",
    "\t\t\t\tindex += 1\n",
    "\n",
    "\t# words counts matrix\n",
    "\tn_doc = len(clean_reviews)\n",
    "\n",
    "\t# record the count of each word occured in each document\n",
    "\tndw = np.zeros((n_doc, num_of_unique_words))\n",
    "\tfor d, review in enumerate(clean_reviews):\n",
    "\t\tfor word in review:\n",
    "\t\t\tl = word_index[word]\n",
    "\t\t\tndw[d, l] += 1\n",
    "\n",
    "\t# words distribution in each topics\n",
    "\tnwz = np.random.rand(num_of_unique_words, num_of_topics)\n",
    "\tpwz = nwz/nwz.sum(axis=0,keepdims=1)\n",
    "\t# the topic distribution for each document\n",
    "\tnzd = np.random.rand(num_of_topics, n_doc)\n",
    "\tpzd = nzd/nzd.sum(axis=0,keepdims=1)\n",
    "\n",
    "\tpzwd = np.zeros((num_of_topics, num_of_unique_words, n_doc))\n",
    "\n",
    "\n",
    "\tfor i in range(num_of_iterations):\n",
    "\t\t# E-step\n",
    "\t\tfor w in range(num_of_unique_words):\n",
    "\t\t\tfor d in range(len(clean_reviews)): \n",
    "\t\t\t\t### very very important condition !ss\n",
    "\t\t\t\tif np.dot(pwz[w,:], pzd[:,d]) == 0:\n",
    "\t\t\t\t\tpzwd[:,w,d] = np.zeros(num_of_topics)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpzwd[:,w,d] = np.multiply(pwz[w,:], pzd[:,d]) / np.dot(pwz[w,:], pzd[:,d])\n",
    "\n",
    "\t\t# M-step\n",
    "\t\t# update pwz\n",
    "\t\tfor k in range(num_of_topics): \n",
    "\t\t\tfor w in range(num_of_unique_words):\n",
    "\t\t\t\tpwz[w,k] = np.matmul(ndw[:,w], pzwd[k,w,:])\n",
    "\t\t\tif np.sum(pwz[:,k]) == 0:\n",
    "\t\t\t\tpwz[:,k] = np.zeros(n_doc)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpwz[:,k] = pwz[:,k] / np.sum(pwz[:,k])\n",
    "\n",
    "\n",
    "\t\t# update pzds\n",
    "\t\tfor d in range(n_doc):\n",
    "\t\t\tfor k in range(num_of_topics):\n",
    "\t\t\t\tpzd[k,d] = np.matmul(ndw[d,:], pzwd[k,:,d]) \n",
    "\t\t\tif np.sum(pzd[:,d]) == 0:\n",
    "\t\t\t\tpzd[:,d] = np.zeros(num_of_unique_words)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpzd[:,d] =  pzd[:,d] / np.sum(pzd[:,d])\n",
    "\n",
    "\treturn pwz, pzd, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T02:43:41.104970Z",
     "start_time": "2019-11-09T02:43:41.102262Z"
    }
   },
   "outputs": [],
   "source": [
    "num_of_topics = 3\n",
    "num_of_iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T02:54:51.957200Z",
     "start_time": "2019-11-09T02:43:41.980453Z"
    }
   },
   "outputs": [],
   "source": [
    "pwz, pzd, index_word = plsa(clean_reviews, num_of_topics, num_of_iterations, len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T02:55:00.909405Z",
     "start_time": "2019-11-09T02:55:00.903819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "work call plumb time need fix job juan would pipe\n",
      "Topic: 2\n",
      "dog servic elain time cat groom use great care know\n",
      "Topic: 3\n",
      "restaur time seafood good clam get tabl wait order chowder\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_of_topics):\n",
    "    print(\"Topic:\", i+1)\n",
    "    top_words = pwz[:,i].argsort()[-10:][::-1]\n",
    "    word_list = []\n",
    "    for j in top_words:\n",
    "        word_list.append(index_word[j])\n",
    "    print(\" \".join(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

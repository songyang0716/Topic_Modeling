{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:08.210372Z",
     "start_time": "2019-10-18T23:05:55.069377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:36.379053Z",
     "start_time": "2019-10-18T23:06:36.376481Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "data_dir = \"/Users/yangsong/Desktop/Projects/gitrepo_songyang0716/Topic_Modeling/reviews_small.txt\"\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:36.958783Z",
     "start_time": "2019-10-18T23:06:36.902827Z"
    }
   },
   "outputs": [],
   "source": [
    "# read review texts\n",
    "reviews = []\n",
    "f = open(data_dir, \"r\")\n",
    "for review in f:\n",
    "    reviews.append(review)\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.072839Z",
     "start_time": "2019-10-18T23:06:37.470766Z"
    }
   },
   "outputs": [],
   "source": [
    "# process text\n",
    "# tokenize, lower, remove stop words, stem, then only keep alphabets in the string\n",
    "clean_reviews = []\n",
    "for review in reviews:\n",
    "    s = nltk.word_tokenize(review)\n",
    "    s = [word.lower() for word in s]\n",
    "    s = [word for word in s if not word in set(\n",
    "        nltk.corpus.stopwords.words('english'))]\n",
    "    s = [stemmer.stem(word) for word in s if word.isalpha()]\n",
    "    clean_reviews.append(\" \".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.106261Z",
     "start_time": "2019-10-18T23:06:40.102231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.140277Z",
     "start_time": "2019-10-18T23:06:40.135118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is the second time I've used APlus for plumbing issues. Juan and Miguel once again resolved our problem with ease and efficiency. I was happy to see them waiting for me once I got home from work at 4:07p for my 4p-6 appointment window.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T23:06:40.179739Z",
     "start_time": "2019-10-18T23:06:40.175568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'second time use aplus plumb issu juan miguel resolv problem eas effici happi see wait got home work appoint window'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:59:08.802817Z",
     "start_time": "2019-10-21T02:59:08.799301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(6, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:59:18.979362Z",
     "start_time": "2019-10-21T02:59:18.956597Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract all the unique biterms from the reviews\n",
    "# BTM directly models the word cooccurrence patterns based on biterms\n",
    "# A biterm denotes an unordered unique word-pair co-occurring in a short context, each context in our example is a review\n",
    "biterms = []\n",
    "unique_words = set()\n",
    "for clean_review in clean_reviews:\n",
    "    clean_review = clean_review.split()\n",
    "    review_length = len(clean_review)\n",
    "    cur_review_biterms = set()\n",
    "    for i in range(review_length):\n",
    "        unique_words.add(clean_review[i])\n",
    "        # we use a interval of 5, if two words are disance to each other less than 5 positions, than count as a biterms\n",
    "        for j in range(i+1, min(i+6, len(clean_review))):\n",
    "            cur_review_biterms.add((clean_review[i], clean_review[j]))\n",
    "    biterms.extend(list(cur_review_biterms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T02:59:36.065782Z",
     "start_time": "2019-10-21T02:59:36.061193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24674"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:39:45.670043Z",
     "start_time": "2019-10-21T06:39:45.661485Z"
    }
   },
   "outputs": [],
   "source": [
    "def BTM(biterms, unique_words, num_of_topics, num_of_iterations):\n",
    "    ####################################################################################\n",
    "    ### num_of_topics: number of topics to generate                                  ###\n",
    "    ### number_of_iterations: collapsed gibbs sampling iterations                    ###\n",
    "    ####################################################################################\n",
    "\n",
    "    # constant we set for the LD prior (topic distributions in a document)\n",
    "    DL_ALPHA = 1\n",
    "    # constant we set for the LD prior (word distribution in a topic)\n",
    "    DL_BETA = 0.01\n",
    "    # Number of total biterms\n",
    "    N_BITERMS = len(biterms)\n",
    "\n",
    "    # Assign a random topic for each biterm\n",
    "    n_z = np.random.randint(0, num_of_topics, N_BITERMS)\n",
    "    n_topics = np.bincount(n_z, minlength=num_of_topics)\n",
    "\n",
    "    # Words count over topics\n",
    "    # Key is word, value is an array of topic counts, use the index to indicate the topic 1 to k\n",
    "    n_wz = defaultdict(lambda: np.zeros(num_of_topics))\n",
    "    for index, (w1, w2) in enumerate(biterms):\n",
    "        n_wz[w1][n_z[index]] += 1\n",
    "        n_wz[w2][n_z[index]] += 1\n",
    "\n",
    "    # unlike to LDA model, in the biterm model, each bigram is coming from a specific topic\n",
    "    # biterm_topic = np.zeros((N_BITERMS, num_of_topics))\n",
    "    for iteration in range(num_of_iterations):\n",
    "        print(iteration)\n",
    "        for index, (w1, w2) in enumerate(biterms):\n",
    "            #             cur_topic = n_z[index]\n",
    "            n_wz[w1][n_z[index]] -= 1\n",
    "            n_wz[w2][n_z[index]] -= 1\n",
    "\n",
    "            n_topics[n_z[index]] -= 1\n",
    "            n_w1z = n_wz[w1]\n",
    "            n_w2z = n_wz[w2]\n",
    "\n",
    "            z_posterior = np.zeros(num_of_topics)\n",
    "#             z_posterior = (n_topics + DL_ALPHA) * (n_w1z + DL_BETA) * (n_w2z + DL_BETA) / np.sum(\n",
    "#                 (2 * n_topics + len(unique_words) * DL_BETA) * (2 * n_topics + len(unique_words) * DL_BETA))\n",
    "            for z in range(num_of_topics):\n",
    "                z_posterior[z] = (n_topics[z] + DL_ALPHA) * (n_w1z[z] + DL_BETA) * (n_w2z[z] + DL_BETA) / np.sum(\n",
    "                    (2 * n_topics[z] + len(unique_words) * DL_BETA) * (2 * n_topics[z] + len(unique_words) * DL_BETA))\n",
    "\n",
    "            topic_prob = z_posterior / np.sum(z_posterior)\n",
    "            topic_selection = np.argmax(\n",
    "                np.random.multinomial(n=1, pvals=topic_prob, size=1))\n",
    "            n_z[index] = topic_selection\n",
    "            n_topics[topic_selection] += 1\n",
    "            n_wz[w1][topic_selection] += 1\n",
    "            n_wz[w2][topic_selection] += 1\n",
    "\n",
    "    # return the topic assignment for each biterm\n",
    "    return n_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:40:01.951185Z",
     "start_time": "2019-10-21T06:39:58.060920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "review = BTM(biterms, unique_words, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:40:03.649297Z",
     "start_time": "2019-10-21T06:40:03.644697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T06:40:05.331944Z",
     "start_time": "2019-10-21T06:40:05.326510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 24674,     0])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(review, minlength=num_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

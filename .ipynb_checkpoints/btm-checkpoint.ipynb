{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:29.859536Z",
     "start_time": "2019-10-23T05:47:11.222086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:29.870121Z",
     "start_time": "2019-10-23T05:47:29.866777Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "data_dir = \"/Users/yangsong/Desktop/Projects/gitrepo_songyang0716/Topic_Modeling/reviews_small.txt\"\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:29.943744Z",
     "start_time": "2019-10-23T05:47:29.877357Z"
    }
   },
   "outputs": [],
   "source": [
    "# read review texts\n",
    "reviews = []\n",
    "f = open(data_dir, \"r\")\n",
    "for review in f:\n",
    "    reviews.append(review)\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.664468Z",
     "start_time": "2019-10-23T05:47:29.954296Z"
    }
   },
   "outputs": [],
   "source": [
    "# process text\n",
    "# tokenize, lower, remove stop words, stem, then only keep alphabets in the string\n",
    "clean_reviews = []\n",
    "for review in reviews:\n",
    "    s = nltk.word_tokenize(review)\n",
    "    s = [word.lower() for word in s]\n",
    "    s = [word for word in s if not word in set(\n",
    "        nltk.corpus.stopwords.words('english'))]\n",
    "    s = [stemmer.stem(word) for word in s if word.isalpha()]\n",
    "    clean_reviews.append(\" \".join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.681692Z",
     "start_time": "2019-10-23T05:47:32.672203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.694440Z",
     "start_time": "2019-10-23T05:47:32.690697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We came in with a large group and ordered a lot of things. There were shrimp. Oysters. There was calamari (yum!). There were prawns and scallops. There was a chicken piccatta. And honestly, some of it all runs together because we're quite the share everything crowd. But I do remember there being a serious cioppino on the menu, and I tried a bit of the sauce. Spicy, rich, and full of seafood. This is a project, but a tasty one.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.707121Z",
     "start_time": "2019-10-23T05:47:32.702914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'came larg group order lot thing shrimp oyster calamari yum prawn scallop chicken piccatta honest run togeth quit share everyth crowd rememb serious cioppino menu tri bit sauc spici rich full seafood project tasti one'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.720426Z",
     "start_time": "2019-10-23T05:47:32.715984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(6, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.751560Z",
     "start_time": "2019-10-23T05:47:32.729000Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract all the unique biterms from the reviews\n",
    "# BTM directly models the word cooccurrence patterns based on biterms\n",
    "# A biterm denotes an unordered unique word-pair co-occurring in a short context, each context in our example is a review\n",
    "biterms = []\n",
    "unique_words = set()\n",
    "for clean_review in clean_reviews:\n",
    "    clean_review = clean_review.split()\n",
    "    review_length = len(clean_review)\n",
    "    cur_review_biterms = set()\n",
    "    for i in range(review_length):\n",
    "        unique_words.add(clean_review[i])\n",
    "        # we use a interval of 5, if two words are disance to each other less than 5 positions, than count as a biterms\n",
    "        for j in range(i+1, min(i+6, len(clean_review))):\n",
    "            cur_review_biterms.add((clean_review[i], clean_review[j]))\n",
    "    biterms.extend(list(cur_review_biterms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.775887Z",
     "start_time": "2019-10-23T05:47:32.772410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24674"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:47:32.805857Z",
     "start_time": "2019-10-23T05:47:32.797981Z"
    }
   },
   "outputs": [],
   "source": [
    "def BTM(biterms, unique_words, num_of_topics, num_of_iterations):\n",
    "    ####################################################################################\n",
    "    ### num_of_topics: number of topics to generate                                  ###\n",
    "    ### number_of_iterations: collapsed gibbs sampling iterations                    ###\n",
    "    ####################################################################################\n",
    "\n",
    "    # constant we set for the LD prior (topic distributions in a document)\n",
    "    DL_ALPHA = 1\n",
    "    # constant we set for the LD prior (word distribution in a topic)\n",
    "    DL_BETA = 0.01\n",
    "    # Number of total biterms\n",
    "    N_BITERMS = len(biterms)\n",
    "\n",
    "    # Assign a random topic for each biterm\n",
    "    n_z = np.random.randint(0, num_of_topics, N_BITERMS)\n",
    "    n_topics = np.bincount(n_z, minlength=num_of_topics)\n",
    "\n",
    "    # Words count over topics\n",
    "    # Key is word, value is an array of topic counts, use the index to indicate the topic 1 to k\n",
    "    n_wz = defaultdict(lambda: np.zeros(num_of_topics))\n",
    "    for index, (w1, w2) in enumerate(biterms):\n",
    "        n_wz[w1][n_z[index]] += 1\n",
    "        n_wz[w2][n_z[index]] += 1\n",
    "\n",
    "    # unlike to LDA model, in the biterm model, each bigram is coming from a specific topic\n",
    "    # biterm_topic = np.zeros((N_BITERMS, num_of_topics))\n",
    "    for iteration in range(num_of_iterations):\n",
    "        for index, (w1, w2) in enumerate(biterms):\n",
    "            #             cur_topic = n_z[index]\n",
    "            n_wz[w1][n_z[index]] -= 1\n",
    "            n_wz[w2][n_z[index]] -= 1\n",
    "\n",
    "            n_topics[n_z[index]] -= 1\n",
    "            n_w1z = n_wz[w1]\n",
    "            n_w2z = n_wz[w2]\n",
    "\n",
    "            z_posterior = np.zeros(num_of_topics)\n",
    "#             z_posterior = (n_topics + DL_ALPHA) * (n_w1z + DL_BETA) * (n_w2z + DL_BETA) / np.sum(\n",
    "#                 (2 * n_topics + len(unique_words) * DL_BETA) * (2 * n_topics + len(unique_words) * DL_BETA))\n",
    "            for z in range(num_of_topics):\n",
    "                z_posterior[z] = (n_topics[z] + DL_ALPHA) * (n_w1z[z] + DL_BETA) * (n_w2z[z] + DL_BETA) / np.sum(\n",
    "                    (2 * n_topics[z] + len(unique_words) * DL_BETA) * (2 * n_topics[z] + len(unique_words) * DL_BETA))\n",
    "\n",
    "            topic_prob = z_posterior / np.sum(z_posterior)\n",
    "            topic_selection = np.argmax(\n",
    "                np.random.multinomial(n=1, pvals=topic_prob, size=1))\n",
    "            n_z[index] = topic_selection\n",
    "            n_topics[topic_selection] += 1\n",
    "            n_wz[w1][topic_selection] += 1\n",
    "            n_wz[w2][topic_selection] += 1\n",
    "\n",
    "   # return the topic assignment for each biterm and the topic distribution of each bigram\n",
    "    return n_z, n_wz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:54:37.169831Z",
     "start_time": "2019-10-23T05:53:48.291676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "n_z, n_wz = BTM(biterms, unique_words, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:56:27.663070Z",
     "start_time": "2019-10-23T05:56:27.659281Z"
    }
   },
   "outputs": [],
   "source": [
    "# The topic distribution of the whole copus is\n",
    "DL_ALPHA = 1\n",
    "num_of_topics = 3\n",
    "topic_distribution = (np.bincount(n_z, minlength=num_of_topics) + DL_ALPHA) / (len(biterms) + num_of_topics * DL_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:56:39.926508Z",
     "start_time": "2019-10-23T05:56:39.922352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32062244, 0.46099607, 0.21838149])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T05:56:49.242075Z",
     "start_time": "2019-10-23T05:56:49.231491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
